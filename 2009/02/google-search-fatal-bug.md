# 从 Google 搜索的严重 bug 想开去 

-----

<div class="post-body entry-content">
　　昨天晚上用 Google 查资料，看到所有的搜索结果都注明是恶意站点。第一反应以为自己眼花了；然后开始怀疑我的浏览器出问题，换了几个浏览器都是老样子；后来又想，会不会是我的 ISP 在搞鬼。等了十多分钟没解决，只好换百度查资料...<a name="more"></a><br/>
　　今天早上看新闻才知道居然是 Google 的【人为】失误，居然是全球性的，居然持续了40多分钟。以下是 Google 的官方说法（<a href="https://googleblog.blogspot.com/2009/01/this-site-may-harm-your-computer-on.html" rel="nofollow" target="_blank">原文连接</a>）：<br/>
<blockquote>谷歌表示他们一直在和一家非盈利组织 StopBadware.org 合作，通过这个组织给出的名单对各个URL进行存在恶意软件的信息标注。这个组织的恶意网站名单是人工审核并添加，所以这份名单是人为生成而不是算法生成的。谷歌昨晚进行列表更新时，在下载回来的名单里，错误的出现了"/"这个URL地址，所以错误的将所有地址判断为恶意网站。</blockquote>　　比较有戏剧性的是，Google 提到的“非盈利组织 StopBadware.org”也发表声明：<br/>
<q>谷歌的恶意网站列表并非是我们提供的，而是其自己生成的。</q><br/>
<br/>
　　这件事情改变了我对 Google 的几个看法。<br/>
　　第1点：虽然我很看不起中国谷歌，但是美国 Google 在我看来一直是一个<b>优秀</b>的公司。现在它居然出现这么低级的错误（开发过商业Web网站的，应该都知道这有多低级），而且持续40多分钟才搞定。看来 Google“<b>优秀</b>”的名声要打折扣了。<br/>
　　第2点：假如 StopBadware 的说法成立，那我就得怀疑 Google 的诚信了。是否 Google 有意推卸责任给其它组织机构？看来 Google“<b>不做恶</b>”的名声要打折扣了。<br/>
　　第3点：Google 一直声称他的所有数据收集、处理、存储都是依靠程序进行的，不会涉及人工干预。我以前也一直猜测 Google 的恶意站点列表是依靠诸如神经网络、遗传算法、等等牛逼的算法来搞定的，现在 Google 居然承认是靠手工审核添加。看来 Google“<b>不采用人工干预</b>”的名声要打折扣了。<br/>
　　第4点：Google 一直在推动云计算，希望大伙儿尽可能地把桌面应用搬上云端。我本来觉得云计算这玩意也挺不错的（优点多多）。现在出了这档子事，我就得谨慎考虑一下了。假如我的个人数据都委托给 Google 代管，万一 Google 出个什么纰漏（比如被 Hacker 搞了），那可就悬了。<br/>
</div>


------------------------------------------------

版权声明本博客所有的原创文章，作者皆保留版权。转载必须包含本声明，保持本文完整，并以超链接形式注明作者编程随想和本文原始地址：https://program-think.blogspot.com/2009/02/google-search-fatal-bug.html
